{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 1 по теме: определение тональной окраски отзыва с помощью словаря"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт необходимых модулей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from random import randint\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import collections\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pymystem3 import Mystem\n",
    "from natasha import (Segmenter, NewsMorphTagger, NewsEmbedding, Doc, NewsSyntaxParser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем UserAgent, который создаёт имитацию браузера, что бы нас не поймали. Для этого же time.sleep(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ua = UserAgent(verify_ssl=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции для выкачивания отзывов и оценок (только 1/5, 2/5 и 5/5, чтобы можно было поделить на две группы) пользователей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_parse(new_href):\n",
    "    while new_href is not None:\n",
    "        rest_url = main_url + new_href\n",
    "        time.sleep(1)\n",
    "        req = requests.get(rest_url, headers={'User-Agent': ua.random})\n",
    "        rest = req.text\n",
    "        soup = BeautifulSoup(rest, 'html.parser')\n",
    "        new_href = soup.find('a', {'class': 'nav next ui_button primary cx_brand_refresh_phase2'})\n",
    "        if new_href is not None:\n",
    "            new_href = new_href['href']\n",
    "        review_blocks = soup.find_all('div', {'class': 'ui_column is-9'})\n",
    "        for r in review_blocks:\n",
    "            com = comments(str(r))\n",
    "            if com[1] == 5:\n",
    "                good.append(com[0])\n",
    "            elif com[1] == 1 or com[1] == 2:\n",
    "                bad.append(com[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments(rev):\n",
    "    soup = BeautifulSoup(rev, 'html.parser')\n",
    "    text = soup.find('p', {'class': 'partial_entry'}).get_text()\n",
    "    mark = soup.find('span')\n",
    "    mark = mark['class'][1]\n",
    "    mark = int(re.sub(r'bubble_(\\d)0', r'\\1', mark))\n",
    "    return text, mark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С сайта https://www.tripadvisor.ru выкачиваем отзывы о тёх ресторанах сети \"Тануки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://www.tripadvisor.ru'\n",
    "good = []\n",
    "bad = []\n",
    "first = ['/Restaurant_Review-g298484-d3775519-Reviews-Tanuki-Moscow_Central_Russia.html',\n",
    "        '/Restaurant_Review-g298484-d3236588-Reviews-Tanuki-Moscow_Central_Russia.html',\n",
    "        '/Restaurant_Review-g298484-d3200244-Reviews-Tanuki-Moscow_Central_Russia.html']\n",
    "for f in first:\n",
    "    page_parse(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего положительных и отрицательных отзывов для составления списков — по 90. Делим выборки. Random, чтобы отзывы о разных ресторанах, оставленные в разное время, перемешались. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(good)\n",
    "random.shuffle(bad)\n",
    "good_list = good[0:90]\n",
    "bad_list = bad[0:90]\n",
    "good_test = good[90:105]\n",
    "bad_test = bad[90:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация, лемматизация, приведение к нижнему регистру."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Новая функция: добавление в словарь синтаксических групп 3 типов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы собрали группы 3 типов. Все они включают глаголы, так как они обычно передают самую основуную суть высказывания. Первая группа (ADV + V, типа \"классно посидели\") лучше всего подходит, потому что она содержит наречие, описывающее действие (и нередко дающее ему оценку!). Вторая группа ('не' + V + S, типа \"не понравилась еда\") включает предикатные группы с отрицанием и прямым дополнением. Эта группа может помочь для определения отрицательной тональности, за счёт включения отрицания. Третья группа (SPRO + V, типа \"нас отравили\", \"нас угостили\") может включать оценку, так как содержит личное местоимение (соответственно, говорящий часто может говорить о себе)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synt_groups(text):   \n",
    "    segmenter = Segmenter()\n",
    "    emb = NewsEmbedding()\n",
    "    syntax_parser = NewsSyntaxParser(emb)\n",
    "    doc = Doc(text.lower())\n",
    "    doc.segment(segmenter)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    syntax_list = []\n",
    "    for el in doc.tokens:\n",
    "        if el.rel != 'punct':\n",
    "            syntax_list.append(tuple([el.text, el.id, el.head_id]))\n",
    "    m = Mystem()\n",
    "    ana = m.analyze(text)\n",
    "    mystem_variant = []\n",
    "    for a in ana:\n",
    "        if 'analysis' in a and a['analysis'] != []:\n",
    "            pos = a['analysis'][0]['gr'].split(',')\n",
    "            pos = pos[0]\n",
    "            if '=' in pos:\n",
    "                pos = pos.split('=')\n",
    "                pos = pos[0]\n",
    "            word = a['text']\n",
    "            wp = tuple([word, pos])\n",
    "            mystem_variant.append(wp)\n",
    "    counter = 0\n",
    "    collocs = []\n",
    "    for morph, synt in zip(mystem_variant, syntax_list):\n",
    "        if (counter != 0) and ((counter + 1) < len(mystem_variant)):\n",
    "            if morph[1] == 'V' and mystem_variant[counter-1][1] == 'ADV' and synt[1] == syntax_list[counter-1][2]:\n",
    "                c = mystem_variant[counter-1][0] + ' ' + morph[0]\n",
    "                collocs.append(c) # ADV + V, типа \"классно посидели\"\n",
    "            elif morph[1] == 'V' and mystem_variant[counter-1][0] == 'не':\n",
    "                if mystem_variant[counter+1][1] == 'S' and synt[1] == syntax_list[counter+1][2]:\n",
    "                    c = 'не' + ' ' + morph[0] + ' ' + mystem_variant[counter+1][0]\n",
    "                    collocs.append(c) # 'не' + V + S, типа \"не понравилась еда\"\n",
    "            elif morph[1] == 'V' and mystem_variant[counter-1][1] == 'SPRO' and synt[1] == syntax_list[counter-1][2]:\n",
    "                c = mystem_variant[counter-1][0] + ' ' + morph[0]\n",
    "                collocs.append(c) # SPRO + V, типа \"нас отравили\", \"нас угостили\"\n",
    "        counter += 1\n",
    "    return collocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(text):\n",
    "    text_words = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "    new_lst = []\n",
    "    morph = MorphAnalyzer()\n",
    "    for elem in text_words:\n",
    "        new_lst.append(morph.parse(elem)[0].normal_form)\n",
    "    new_lst.extend(synt_groups(text)) # Accuracy без этой строки равна 0,8.\n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составление двух списков уникальных слов из отзывов положительной и отрицательной групп. Фильтр отсеивает шум (стоп-слова и слова, которые встречаются 1 раз)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_lemmas(lst):\n",
    "    if type(lst) == list:\n",
    "        lst = ' '.join(lst)\n",
    "    new_lst = preproc(lst)\n",
    "    count = collections.Counter(new_lst).most_common()\n",
    "    final_lst = []\n",
    "    for c in count:\n",
    "        if c[1] > 1:\n",
    "            final_lst.append(c[0])\n",
    "    sw = stopwords.words('russian')\n",
    "    final_lst = [w for w in final_lst if w not in sw]\n",
    "    return final_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_list = set(tokens_lemmas(good_list))\n",
    "bad_list = set(tokens_lemmas(bad_list))\n",
    "both = good_list & bad_list\n",
    "good_list = list(good_list - both)\n",
    "bad_list = list(bad_list - both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция определения тональности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tonality(sent):\n",
    "    sent_lst = preproc(sent)\n",
    "    k = 0\n",
    "    for w in sent_lst:\n",
    "        if w in good_list:\n",
    "            k += 1\n",
    "        elif w in bad_list:\n",
    "            k -= 1\n",
    "    if k > 0:\n",
    "        return 1\n",
    "    elif k < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return randint(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчёт accuracy с помощью тестовой выборки (15 положительных и 15 отрицательных отзывов). **Итоговый результат: для программы без синтаксических групп (ниже) — 0,8, с ними (сразу под текущей ячейкой) — 0,83.** Таким образом, мы видим, что включение синтаксических групп положительно повлияло (причём, 0,03 - это не такая уж и маленькая разница) на качество работы программы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.83\n"
     ]
    }
   ],
   "source": [
    "real = []\n",
    "test = []\n",
    "for g in good_test:\n",
    "    real.append(1)\n",
    "    test.append(tonality(g))\n",
    "for b in bad_test:\n",
    "    real.append(0)\n",
    "    test.append(tonality(b))\n",
    "print('Accuracy =', round(accuracy_score(real, test), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8\n"
     ]
    }
   ],
   "source": [
    "real = []\n",
    "test = []\n",
    "for g in good_test:\n",
    "    real.append(1)\n",
    "    test.append(tonality(g))\n",
    "for b in bad_test:\n",
    "    real.append(0)\n",
    "    test.append(tonality(b))\n",
    "print('Accuracy =', round(accuracy_score(real, test), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение программы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Можно смотреть не собственно слова, а их векторные представления (с помощью предобученной модели RusVectores, так как обучать модель для этой задачи незачем). Оценочная лексика может разбиться на группы в зависимоти от её тональности. В тестовой выборке мы могли бы сравнивать не слова, а векторые представления. Это было бы полезно в случае с синонимами, то есть, если у нас в списке есть слово \"отвратительный\", но нет \"ужасный\", то для нашей программы слово \"ужасный\" теряется, а вот векторные представления этих слов были бы достаточно близки, что помогло бы в опредлении тональности отзыва.\n",
    "2. С помощью нейросетей можно провести оценку тональности эмодзи, которых, кстати, в отзывах очень много."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synt_groups(text):   \n",
    "    segmenter = Segmenter()\n",
    "    emb = NewsEmbedding()\n",
    "    syntax_parser = NewsSyntaxParser(emb)\n",
    "    doc = Doc(text.lower())\n",
    "    doc.segment(segmenter)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    syntax_list = []\n",
    "    for el in doc.tokens:\n",
    "        if el.rel != 'punct':\n",
    "            syntax_list.append(tuple([el.text, el.id, el.head_id]))\n",
    "    print(len(syntax_list))\n",
    "    m = Mystem()\n",
    "    ana = m.analyze(text)\n",
    "    mystem_variant = []\n",
    "    for a in ana:\n",
    "        if 'analysis' in a:\n",
    "            pos = a['analysis'][0]['gr'].split(',')\n",
    "            pos = pos[0]\n",
    "            if '=' in pos:\n",
    "                pos = pos.split('=')\n",
    "                pos = pos[0]\n",
    "            pos = mystem_convert(pos)\n",
    "            word = a['text']\n",
    "            wp = tuple([word, pos])\n",
    "            mystem_variant.append(wp)\n",
    "    counter = 0\n",
    "    collocs_1 = [] # ADV + V, типа \"классно посидели\"\n",
    "    collocs_2 = [] # 'не' + V + S, типа \"не понравилась еда\"\n",
    "    collocs_3 = [] # SPRO + V, типа \"нас отравили\", \"нас угостили\"\n",
    "    for morph, synt in zip(mystem_variant, syntax_list):\n",
    "        if counter != 0:\n",
    "            if morph[1] == 'V' and mystem_variant[counter-1][1] == 'ADV' and synt[1] == syntax_list[counter-1][2]:\n",
    "                c = mystem_variant[counter-1][0] + ' ' + morph[0]\n",
    "                collocs_1.append(c) \n",
    "            elif morph[1] == 'V' and mystem_variant[counter-1][0] == 'не':\n",
    "                if mystem_variant[counter+1][1] == 'S' and synt[1] == syntax_list[counter+1][2]:\n",
    "                    c = 'не' + ' ' + morph[0] + ' ' + mystem_variant[counter+1][0]\n",
    "                    collocs_2.append(c)\n",
    "            elif morph[1] == 'V' and mystem_variant[counter-1][1] == 'SPRO' and synt[1] == syntax_list[counter-1][2]:\n",
    "                c = mystem_variant[counter-1][0] + ' ' + morph[0]\n",
    "                collocs_3.append(c) \n",
    "        counter += 1\n",
    "    return collocs_1, collocs_2, collocs_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
