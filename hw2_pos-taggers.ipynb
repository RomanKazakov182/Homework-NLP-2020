{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2 по теме: сравнение качества POS-теггеров и выделение синтаксических групп"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pymystem3 import Mystem\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag, map_tag\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from natasha import (Segmenter, NewsMorphTagger, NewsEmbedding, Doc, NewsSyntaxParser)\n",
    "import spacy\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rus_benchmark_plain.txt', encoding='utf-8') as fh:\n",
    "    text = fh.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот текст подходит как сложный, так как это отрывки из стихотворений В. Хлебникова (привет словам типа \"крылышкуя\"), здесь есть омонимы типа \"о\" (междометие/предлог), \"суша\" (деепричатие/существительное), \"жил\" (глагол/существительное). Ещё здесь есть старорусский союз \"аль\" и странное звукоподражание \"пинь\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Крылышкуя золотописьмом\n",
      "Тончайших жил,\n",
      "Кузнечик в кузов пуза уложил\n",
      "Прибрежных много трав и вер.\n",
      "«Пинь, пинь, пинь!» — тарарахнул зинзивер.\n",
      "О, лебедиво!\n",
      "О, озари!\n",
      "Гонимый — кем, почем я знаю?\n",
      "Вопросом: поцелуев в жизни сколько?\n",
      "Румынкой, дочерью Дуная,\n",
      "Иль песнью лет про прелесть польки,—\n",
      "Бегу в леса, ущелья, пропасти\n",
      "И там живу сквозь птичий гам,\n",
      "Как снежный сноп, сияют лопасти\n",
      "Крыла, сверкавшего врагам.\n",
      "Хотел бы шляхтичем на сейме,\n",
      "Руку положив на рукоятку сабли,\n",
      "Тому, отсвет желаний чей мы,\n",
      "Крикнуть, чтоб узы воль ослабли.\n",
      "Ты дичишься? что причина?\n",
      "Аль не я рукой одною\n",
      "Удержу на пашне тройку?\n",
      "Аль не я спалил весною\n",
      "Так, со зла, шабра постройку?\n",
      "Костры горят сторожевые\n",
      "На всех священных площадях,\n",
      "И вижу — едут часовые\n",
      "На челнах, лодках и конях.\n",
      "О, если б волосами синих рек\n",
      "Мне Азия покрыла бы колени\n",
      "И дева прошептала таинственные пени,\n",
      "И тихая, счастливая, рыдала,\n",
      "Концом косы глаза суша.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего слов: 144\n"
     ]
    }
   ],
   "source": [
    "benchmark = []\n",
    "with open('rus_benchmark_pos.csv', encoding='utf-8') as fh:\n",
    "    words_pos = csv.reader(fh, delimiter=';')\n",
    "    for el in words_pos:\n",
    "        benchmark.append(tuple(el))\n",
    "print('всего слов:', len(benchmark))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Русский язык"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mystem_convert(tag):\n",
    "    if tag == 'ANUM' or tag == 'APRO':\n",
    "        return 'A'\n",
    "    elif tag == 'SPRO':\n",
    "        return 'PRO'\n",
    "    elif tag == 'ADVPRO':\n",
    "        return 'ADV'\n",
    "    else:\n",
    "        return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mystem_pos(text):\n",
    "    m = Mystem()\n",
    "    ana = m.analyze(text)\n",
    "    mystem_variant = []\n",
    "    for a in ana:\n",
    "        if 'analysis' in a:\n",
    "            pos = a['analysis'][0]['gr'].split(',')\n",
    "            pos = pos[0]\n",
    "            if '=' in pos:\n",
    "                pos = pos.split('=')\n",
    "                pos = pos[0]\n",
    "            pos = mystem_convert(pos)\n",
    "            word = a['text']\n",
    "            wp = tuple([word, pos])\n",
    "            mystem_variant.append(wp)\n",
    "    print('всего слов в анализе Mystem:', len(mystem_variant))\n",
    "    return mystem_variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего слов в анализе Mystem: 144\n",
      "accuracy для Mystem: 0.9236\n"
     ]
    }
   ],
   "source": [
    "print('accuracy для Mystem:', \n",
    "      round(accuracy_score([y[1] for y in benchmark], \n",
    "                           [y[1] for y in mystem_pos(text)]), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pymorhy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pymorhy_convert(tag):\n",
    "    tags = {'NOUN': 'S', 'ADJF': 'A', 'ADJS': 'A', 'COMP': 'A',\n",
    "           'VERB': 'V', 'INFN': 'V', 'PRTF': 'V', 'PRTS': 'V', 'GRND': 'V',\n",
    "           'NUMR': 'NUM', 'ADVB': 'ADV', 'NPRO': 'PRO', 'PREP': 'PR',\n",
    "           'CONJ': 'CONJ', 'PRCL': 'PART', 'INTJ': 'INTJ'}\n",
    "    return tags[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pymorphy_pos(text):\n",
    "    words = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "    morph = MorphAnalyzer()\n",
    "    analize_list = []\n",
    "    for token in words:\n",
    "        token_analized = morph.parse(token)[0]\n",
    "        pos = str(token_analized.tag.POS)\n",
    "        analize_list.append(tuple([token, pymorhy_convert(pos)]))\n",
    "    return analize_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy для Pymorphy: 0.8958\n"
     ]
    }
   ],
   "source": [
    "print('accuracy для Pymorphy:', \n",
    "      round(accuracy_score([y[1] for y in benchmark], \n",
    "                           [y[1] for y in pymorphy_pos(text)]), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natasha_convert(tag):\n",
    "    tags = {'ADJ': 'A', 'ADP': 'PR', 'ADV': 'ADV', 'AUX': 'V', 'CONJ': 'CONJ',\n",
    "           'CCONJ': 'CONJ', 'DET': 'A', 'INTJ': 'INTJ', 'NOUN': 'S', 'NUM': 'NUM',\n",
    "            'PART': 'PART', 'PROPN': 'S', 'PRON': 'PRO', 'SCONJ': 'CONJ', \n",
    "            'VERB': 'V'}\n",
    "    return tags[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natasha_pos(text):\n",
    "    segmenter = Segmenter()\n",
    "    emb = NewsEmbedding()\n",
    "    morph_tagger = NewsMorphTagger(emb)\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    natasha_ana = []\n",
    "    for el in doc.tokens:\n",
    "        if el.pos != 'PUNCT':\n",
    "            natasha_ana.append(tuple([el.text, natasha_convert(el.pos)]))\n",
    "    print('всего слов в анализе Natasha:', len(natasha_ana))\n",
    "    return natasha_ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего слов в анализе Natasha: 144\n",
      "accuracy для Natasha: 0.7778\n"
     ]
    }
   ],
   "source": [
    "print('accuracy для Natasha:', \n",
    "      round(accuracy_score([y[1] for y in benchmark], \n",
    "                           [y[1] for y in natasha_pos(text)]), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось, что для моего Золотого стандарта лучшим морфоанализатором русского языка является Mystem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Английский"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это тексты нескольких твитов Илона Маска. Здесь есть и классическая конверсия типа \"resume\", \"offer\" и других (очень много!). Присутствует сленг  (например, \"booty\"), сокращения (например, \"lmk\"), междометие \"haha\", а также множество имён собственных (\"Tesla\", \"Britain\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We must pass the great filter.\n",
      "The gauntlet has been thrown down! \n",
      "The prophecy will be fulfilled. \n",
      "Will be less roomy with three vacuum rocket engines added.\n",
      "Turn volume to eleven & play Powerglide in your Tesla.\n",
      "Call of booty, great game.\n",
      "We will teach you what’s known about the brain, which is not much.\n",
      "If you feel Neuralink might have incorrectly overlooked your resume or declined to  make an offer, please lmk in comment below.\n",
      "of course I still love you.\n",
      "Turns out you can make anything fly haha.\n",
      "Olde skoole analog synthesizer from ancient Britain.\n",
      "Bureaucracy is inherently kafkaesque.\n",
      "Thanks Tesla owners & investors! Love you!! We will work super hard to earn your trust & support.\n",
      "Who controls the memes, controls the universe.\n"
     ]
    }
   ],
   "source": [
    "with open('eng_benchmark_plain.txt', encoding='utf-8') as fh:\n",
    "    eng_text = fh.read()\n",
    "print(eng_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего слов: 124\n"
     ]
    }
   ],
   "source": [
    "eng_benchmark = []\n",
    "with open('eng_benchmark_pos.csv', encoding='utf-8') as fh:\n",
    "    words_pos = csv.reader(fh, delimiter=';')\n",
    "    for el in words_pos:\n",
    "        eng_benchmark.append(tuple(el))\n",
    "print('всего слов:', len(eng_benchmark))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_pos(eng_text):    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(eng_text)\n",
    "    spacy_ana = []\n",
    "    for token in doc:\n",
    "        if token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and token.text != '&':\n",
    "            spacy_ana.append(tuple([token.text, token.pos_]))\n",
    "    print('всего слов в анализе Spacy:', len(spacy_ana))\n",
    "    return spacy_ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего слов в анализе Spacy: 124\n",
      "accuracy для Spacy: 0.9032\n"
     ]
    }
   ],
   "source": [
    "print('accuracy для Spacy:', \n",
    "      round(accuracy_score([y[1] for y in eng_benchmark], \n",
    "                           [y[1] for y in spacy_pos(eng_text)]), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flair_pos(eng_text):    \n",
    "    sentence = Sentence(eng_text)\n",
    "    tagger = SequenceTagger.load('upos')\n",
    "    tagger.predict(sentence)\n",
    "    flair_ana = sentence.to_tagged_string()\n",
    "    flair_ana = flair_ana.split()\n",
    "    counter = 0\n",
    "    flair_var = []\n",
    "    for el in flair_ana:\n",
    "        if counter % 2 == 0:\n",
    "            if flair_ana[counter+1] != '<PUNCT>' and el != '&':\n",
    "                pos = flair_ana[counter+1]\n",
    "                pos = re.sub(r'<(.+?)>', r'\\1', pos)\n",
    "                if pos == 'AUX':\n",
    "                    pos = 'VERB'\n",
    "                flair_var.append(tuple([el, pos]))\n",
    "        counter += 1\n",
    "    print('всего слов в анализе Flair:', len(flair_var))\n",
    "    return flair_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-18 22:07:25,801 loading file /Users/romankazakov/.flair/models/en-pos-ontonotes-v0.4.pt\n",
      "всего слов в анализе Flair: 124\n",
      "accuracy для Flair: 0.9435\n"
     ]
    }
   ],
   "source": [
    "for_flair_benchmark = []\n",
    "for ana in eng_benchmark:\n",
    "    if ana[1] == 'AUX':\n",
    "        for_flair_benchmark.append(tuple([ana[0], 'VERB']))\n",
    "    else:\n",
    "        for_flair_benchmark.append(ana)\n",
    "print('accuracy для Flair:', \n",
    "      round(accuracy_score([y[1] for y in for_flair_benchmark], \n",
    "                           [y[1] for y in flair_pos(eng_text)]), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk_text = word_tokenize(eng_text)\n",
    "nltk_ana = nltk.pos_tag(nltk_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось, что для моего Золотого стандарта лучшим морфоанализатором английского языка является Flair!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Синтаксические группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synt_groups(text):   \n",
    "    segmenter = Segmenter()\n",
    "    emb = NewsEmbedding()\n",
    "    syntax_parser = NewsSyntaxParser(emb)\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    #print(doc.sents[0].text)\n",
    "    syntax_list = []\n",
    "    for el in doc.tokens:\n",
    "        if el.rel != 'punct':\n",
    "            syntax_list.append(tuple([el.text, el.id, el.head_id]))\n",
    "    print(len(syntax_list))\n",
    "    m = Mystem()\n",
    "    ana = m.analyze(text)\n",
    "    mystem_variant = []\n",
    "    for a in ana:\n",
    "        if 'analysis' in a:\n",
    "            pos = a['analysis'][0]['gr'].split(',')\n",
    "            pos = pos[0]\n",
    "            if '=' in pos:\n",
    "                pos = pos.split('=')\n",
    "                pos = pos[0]\n",
    "            pos = mystem_convert(pos)\n",
    "            word = a['text']\n",
    "            wp = tuple([word, pos])\n",
    "            mystem_variant.append(wp)\n",
    "    counter = 0\n",
    "    collocs_1 = [] # ADV + V, типа \"классно посидели\"\n",
    "    collocs_2 = [] # 'не' + V + S, типа \"не понравилась еда\"\n",
    "    collocs_3 = [] # SPRO + V, типа \"нас отравили\", \"нас угостили\"\n",
    "    for morph, synt in zip(mystem_variant, syntax_list):\n",
    "        if counter != 0:\n",
    "            if morph[1] == 'V' and mystem_variant[counter-1][1] == 'ADV' and synt[1] == syntax_list[counter-1][2]:\n",
    "                c = mystem_variant[counter-1][0] + ' ' + morph[0]\n",
    "                collocs_1.append(c) \n",
    "            elif morph[1] == 'V' and mystem_variant[counter-1][0] == 'не':\n",
    "                if mystem_variant[counter+1][1] == 'S' and synt[1] == syntax_list[counter+1][2]:\n",
    "                    c = 'не' + ' ' + morph[0] + ' ' + mystem_variant[counter+1][0]\n",
    "                    collocs_2.append(c)\n",
    "            elif morph[1] == 'V' and mystem_variant[counter-1][1] == 'SPRO' and synt[1] == syntax_list[counter-1][2]:\n",
    "                c = mystem_variant[counter-1][0] + ' ' + morph[0]\n",
    "                collocs_3.append(c) \n",
    "        counter += 1\n",
    "    return collocs_1, collocs_2, collocs_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустить на материале 1 ДЗ я не успел, но вот функция. Примеры лучше всего объясняют, почему нужны именно такие синтаксические группы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
