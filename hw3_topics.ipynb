{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 3 по теме: определение темы через Gensim и TF_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем необходимые модули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import spacy\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем датасет и преобразуем его в DataFrame. Дальнейший код представляет описывает препроцессинг перед обучением LDA Mallet модели, он, по сути, взят с семинара по Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очищаем тексты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.content.values.tolist()\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация текстов и подготовка модели биграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "data_words = list(sent_to_words(data))\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистка от стоп-слов. Создание биграмм и лемматизация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Топикализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка списков лемм к вводу в модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция нахождения наиболее оптимального числа топиков с помощью *CoherenceModel*. Используемая модель топикализации – *LDAMallet*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_num():\n",
    "    results = []\n",
    "    for num_topics in tqdm(range(1, 50, 5)):\n",
    "        mallet_path = '/Users/romankazakov/Downloads/mallet-2.0.8/bin/mallet'\n",
    "        ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "        results.append(tuple([num_topics, coherence_model_ldamallet.get_coherence()]))\n",
    "    best = (0, 0)\n",
    "    for a in results:\n",
    "        if a[1] > best[1]:\n",
    "            best = a\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [16:37<00:00, 99.72s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таким образом, наиболее оптимальное число топиков: 21 (coherence score равен 0.5375255460843703).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = best_num()\n",
    "print('Таким образом, наиболее оптимальное число топиков:', res[0], \n",
    "      '(coherence score равен', str(res[1]) + ').')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, **оптимальное число топиков — 21**. Теперь создадим ещё одну модель *LDAMallet*, чтобы работать именно с ней."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = '/Users/romankazakov/Downloads/mallet-2.0.8/bin/mallet'\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=21, id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем лишние столбцы из DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['target']\n",
    "del df['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем список всех топиков и по 10 самых \"важных\" слов для каждого из них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_topics = ldamallet.show_topics(num_topics =-1, formatted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью счётчика весов слов, соответствующих определённым топикам, определяем широкие топики для каждого текста и записываем их в DataFrame вместе со списками лемм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for text in texts:\n",
    "    dic = {}\n",
    "    for word in text:\n",
    "        for topic in all_topics:\n",
    "            for key_word in topic[1]:\n",
    "                if key_word[0] == word:\n",
    "                    if topic[0] in dic:\n",
    "                        dic[topic[0]] += key_word[1]\n",
    "                    else:\n",
    "                        dic[topic[0]] = key_word[1]\n",
    "    if dic != {}:\n",
    "        score = max(list(dic.values()))\n",
    "        for k, v in list(dic.items()):\n",
    "            if v == score:\n",
    "                main_topic = k\n",
    "                break\n",
    "    else:\n",
    "        main_topic = np.nan\n",
    "    df.loc[counter, 'topic'] = main_topic\n",
    "    df.loc[counter, 'lemmas'] = [[t] for t in texts[counter]]\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение TF_IDF (метрика, которая показывает насколько важно/релевантно слово в заданном документе). Считается для каждого текста относительно группы текстов по топику. Для каждого документа записываются 5 самых релевантных слов (key-words) в DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [04:54<00:00, 14.03s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(21)):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([' '.join(text) for text in list(df.loc[df['topic'] == i, 'lemmas'])])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df_tfidf = pd.DataFrame(denselist, columns=feature_names).T\n",
    "    df_tfidf.columns = list(df.index[df['topic'] == i])\n",
    "    cols = df_tfidf.columns\n",
    "    for f in cols:\n",
    "        best = df_tfidf[f].sort_values(ascending=False).head().index.tolist()\n",
    "        df.loc[f, 'tf_idf_words'] = [[k] for k in best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('result_mails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Программа создаёт из DataFrame файл .csv, но я не могу прикрепить его на GitHub, потому что он слишком тяжёлый. Поэтому покажу здесь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>topic</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>tf_idf_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[where, s, thing, car, nntp_posting, host, lin...</td>\n",
       "      <td>[car, door, lerxst, funky, front_bumper]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[guy_kuo, summary, final, call, report, accele...</td>\n",
       "      <td>[upgrade, clock, guy_kuo, add, experience]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[defence, technology, organisation, line, writ...</td>\n",
       "      <td>[fractally, recognisable, tv_station, organisa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>[earl_wallace, friend, platform, software, lin...</td>\n",
       "      <td>[gun, people, bulky, allowed, earl_wallace]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[organization, line, host, write, show, exampl...</td>\n",
       "      <td>[domestication, behavior, wild, domesticate, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>From: a207706@moe.dseg.ti.com (Robert Loper)\\n...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[write, help, wife, inform, wants_convertible,...</td>\n",
       "      <td>[convertible, car, wife, range, ds]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>From: kimman@magnus.acs.ohio-state.edu (Kim Ri...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[steven_walz, many, homosexual, organization, ...</td>\n",
       "      <td>[law, charge, beat, deter, straight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>From: kwilson@casbah.acns.nwu.edu (Kirtley Wil...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[survivor, reply, distribution, bench, line, w...</td>\n",
       "      <td>[blast, fatigue, people, knock, hell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>Subject: Re: Don't more innocents die without ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[loral, line, write, hate, bike, start, week, ...</td>\n",
       "      <td>[plastic, compound, rubbing, work, loral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>From: livesey@solntze.wpd.sgi.com (Jon Livesey...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[game, game, game, line, electronic, art, disk...</td>\n",
       "      <td>[game, best_offer, art, electronic, original]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>From: dls@aeg.dsto.gov.au (David Silver)\\nSubj...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[line, reply, external, wish, startup, disk, p...</td>\n",
       "      <td>[boot, external, warm, startup, reply]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>Subject: Re: Mike Francesa's 1993 Predictions\\...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>[flame, summary, be, outta, begin, week, sabba...</td>\n",
       "      <td>[ride, week, fly, cfc, newsfee]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>From: jet@netcom.Netcom.COM (J. Eric Townsend)...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[line, article, write, article, write, questio...</td>\n",
       "      <td>[redneck, fascinate, ride, spluttering, southe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>From: gld@cunixb.cc.columbia.edu (Gary L Dare)...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>[line, turn, drive, sea, sick, tired, driving,...</td>\n",
       "      <td>[palestinian, islamic, situation, find, choice]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>From: sehari@iastate.edu (Babak Sehari)\\nSubje...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[machine, trust, host, com, machine, trial, na...</td>\n",
       "      <td>[imagine, machine, nanosecond, mjr, universe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>From: danmg@grok85.ColumbiaSC.NCR.COM (Daniel ...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[lake_steven, wa, line, think, result, last, w...</td>\n",
       "      <td>[mark, cheryl, banner, lake_steven, wa]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>From: henry@zoo.toronto.edu (Henry Spencer)\\nS...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[people, marry, line, article, write, think, s...</td>\n",
       "      <td>[couple, marry, marriage, priest, community]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>From: tzs@stein2.u.washington.edu (Tim Smith)\\...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[work, great, article, write, weight_rebound, ...</td>\n",
       "      <td>[weight, term, lose, weight_rebound, constitut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>From: U56149@uicvm.uic.edu\\nSubject: LCIII &amp; M...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[host, sydney_australia, line, naive, question...</td>\n",
       "      <td>[computationally, intensive, job, claim, lino]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>From: nsmca@aurora.alaska.edu\\nSubject: Lunar ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[interesting, case, school, engineering, line,...</td>\n",
       "      <td>[mouse, quirk, speed, style, normal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>From: acooper@mac.cc.macalstr.edu\\nSubject: Re...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[standing, organization, ericsson_telecom, lin...</td>\n",
       "      <td>[relegation, quarterfinal, standing, swedish_h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10015</th>\n",
       "      <td>From: billc@col.hp.com (Bill Claussen)\\nSubjec...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[organization, line, reference, line, trim, wr...</td>\n",
       "      <td>[murder, moral, system, morality, goal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10016</th>\n",
       "      <td>From: vestman@cs.umu.se (Peter Vestman)\\nSubje...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[reader, info, particle_physic, line, help, ca...</td>\n",
       "      <td>[reader, connector, locally, particle_physic, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10017</th>\n",
       "      <td>From: nstramer@supergas.dazixco.ingr.com (Naft...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[year, big, worst_opinion, keyword, article, a...</td>\n",
       "      <td>[season, team, goal, point, hartford_whaler]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10018</th>\n",
       "      <td>From: kohli@ecs.umass.edu\\nSubject: Mazda GLC ...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[periodically, hang, application, openwindow, ...</td>\n",
       "      <td>[application, openwindow, click, pulldown, ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>From: mussack@austin.ibm.com (Christopher Muss...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[insurance, lotsa_point, nntp_posting, host, a...</td>\n",
       "      <td>[drunk, insure, drive, insurance, vehicle]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[region, warrior, ticket, line, distribution, ...</td>\n",
       "      <td>[ticket, warrior, opponent, row, region]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10020</th>\n",
       "      <td>From: noye@midway.uchicago.edu (vera shanti no...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[ear, world, public_access, line, victor_lakin...</td>\n",
       "      <td>[sighting, planet, einstein, ear, explain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10021</th>\n",
       "      <td>From:  (Sean Garrison)\\nSubject: Re: WFAN\\nNnt...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[protece, tromsoe, line, be, look, information...</td>\n",
       "      <td>[proteced, protece, tromsoe, mode, support]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10022</th>\n",
       "      <td>From: christy@cs.concordia.ca (Christy)\\nSubje...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[line, article, write, article, gregg_jaeger, ...</td>\n",
       "      <td>[islamic, statement, country, rushdie, publica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>From: rash@access.digex.com (Wayne Rash)\\nSubj...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[model, organization, line, do, model, pin, ve...</td>\n",
       "      <td>[model, do, pin, job, already]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>From: gt0523e@prism.gatech.EDU (Michael Andre ...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[dept, electrical_engineere, line, add, postag...</td>\n",
       "      <td>[eastw, electrical_engineere, shrink, disc, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>From: dbernard@clesun.Central.Sun.COM (Dave Be...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[gun, advertisement, organization, laboratory,...</td>\n",
       "      <td>[ad, sportshop, crpa, sportsmen, examiner]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>From: smk5@quads.uchicago.edu (Steve Kramarsky...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[documentation, article, line, find, believe, ...</td>\n",
       "      <td>[chak, undocumented, legend, diagnostic, mapping]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>From: ob00@ns1.cc.lehigh.edu (OLCAY BOZ)\\nSubj...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[people, become, atheist, reply, write, articl...</td>\n",
       "      <td>[atheist, religion, belief, hostility, believe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>From: sera@zuma.UUCP (Serdar Argic)\\nSubject: ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[douglas_rand, draw, line, inverse, message, o...</td>\n",
       "      <td>[gxxor, inverse, draw, terminal, white]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>From: moskowit@panix.com (Len Moskowitz)\\nSubj...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[organization, apply, line, follow, comment, a...</td>\n",
       "      <td>[cdt, koresh, claim, clh, prophet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>From: rws2v@uvacs.cs.Virginia.EDU (Richard Sto...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[article, write, write, tiny, area, land, high...</td>\n",
       "      <td>[population_density, statement, gaza_strip, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>From: geb@cs.pitt.edu (Gordon Banks)\\nSubject:...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[write, article, write, friend, connect, use, ...</td>\n",
       "      <td>[procomm_plus, baud_modem, trusty, menzel, mac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>From: keith@cco.caltech.edu (Keith Allan Schne...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[muslim, cruelly, bayonet, death, reply, serda...</td>\n",
       "      <td>[armenian, genocide, massacre, russian, turkish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>From: bredell@tdb.uu.se (Mats Bredell)\\nSubjec...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[update, organization, write, result, day, vot...</td>\n",
       "      <td>[vote, mould, valerie_hammerl, voting, philly]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>From: cantrell@sauron.msfc.nasa.gov (Eric Cant...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[s, wrong, host, have, also, find, electronic,...</td>\n",
       "      <td>[fixture, nuke, instant, lamp, starter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>From: cosmo@pro-angmar.alfalfa.com (Frank Bens...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>[standing, organization, hewlett_packard, line...</td>\n",
       "      <td>[blue, dirty, team, standing, white]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>From: as010b@uhura.cc.rochester.edu (Tree of S...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[lesc, line, host, antare, ncd, exceed, softwa...</td>\n",
       "      <td>[antare, lesc, exceed, ncd, opinions_expresse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>From: Thyagi@cup.portal.com (Thyagi Morgoth Na...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[need, cheap, drug, line, write, answer, seem,...</td>\n",
       "      <td>[cheap, crypto, birthright, clipper_chip, stro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>From: leech@cs.unc.edu (Jon Leech)\\nSubject: S...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[want, super, projector, sound, projector, nnt...</td>\n",
       "      <td>[projector, sound, super, sale, danny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>From: dmp1@ukc.ac.uk (D.M.Procida)\\nSubject: R...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[line, article, write, write, nuclear, fission...</td>\n",
       "      <td>[uranium, cooling_tower, air, mist, melt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>From: JBE5 &lt;JBE5@MUSICB.MCGILL.CA&gt;\\nSubject: W...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[armenian, problem, write, tell, follow, story...</td>\n",
       "      <td>[persian, persian_troop, persian_army, fourth,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>From: kurt@oddjob.uchicago.edu (Kurt Henriksen...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[problem, organization, western_geophysical, e...</td>\n",
       "      <td>[doug, libtermcap_termcap, exploration_product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>From: dxf12@po.cwru.edu (Douglas Fowler)\\nSubj...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[clipper_chip, crypto, organization, line, hos...</td>\n",
       "      <td>[encryption, scrambled, technology, wedge, lull]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>From: dkfox@uxa.cso.uiuc.edu (fox darin k)\\nSu...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[love, royal_road, military, line, article, wr...</td>\n",
       "      <td>[seal, btw, faith, dk, took]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>From: ez005997@othello.ucdavis.edu (Oppy)\\nSub...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[article, write, way, want, however, follow, p...</td>\n",
       "      <td>[helmet, large, hang, chin, size]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>From: mont@netcom.com (Mont Pierce)\\nSubject: ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[signal_surge, home, nntp_posting, host, write...</td>\n",
       "      <td>[unlikely, possible, signal_surge, alternator,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>From: bskendig@netcom.com (Brian Kendig)\\nSubj...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[school, engineering, line, host, useragent_nu...</td>\n",
       "      <td>[defect, manufacture, monitor, new, school]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>From: carl@lvsun.com (Carl Shapiro)\\nSubject: ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[organization, mantis_consultant, line, write,...</td>\n",
       "      <td>[recursive, recursiveness, iterative, optimizi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>From: sanjay@kin.lap.upenn.edu (Sanjay Sinha)\\...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[gordon_bank, gordon, objection, article, writ...</td>\n",
       "      <td>[gordon_bank, sin, blood, mormon, save]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Organization: Central Michigan University\\nFro...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>[article, line, need, shelf, method, transmit,...</td>\n",
       "      <td>[transmitting, product, prefered, appreciate_h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>From: cr292@cleveland.Freenet.Edu (Jim Schenk)...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[line, think, get, consumer_report, rank, high...</td>\n",
       "      <td>[car, deal, fully_loade, catagorie, honda_accord]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>From: wbdst+@pitt.edu (William B Dwinnell)\\nSu...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[line, sale, fit, car, cover, specifically, to...</td>\n",
       "      <td>[cover, fabric, grommet, blemish, stain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>From: ehung@ampex.com (Eric Hung)\\nSubject: Re...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>[line, allegro, sophisticated, design, straigh...</td>\n",
       "      <td>[sheet, sophisticated, cod, preference, never]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  topic  \\\n",
       "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...   11.0   \n",
       "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...    9.0   \n",
       "10     From: irwin@cmptrc.lonestar.org (Irwin Arnstei...    6.0   \n",
       "100    From: tchen@magnus.acs.ohio-state.edu (Tsung-K...   16.0   \n",
       "1000   From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...    6.0   \n",
       "10000  From: a207706@moe.dseg.ti.com (Robert Loper)\\n...    6.0   \n",
       "10001  From: kimman@magnus.acs.ohio-state.edu (Kim Ri...    6.0   \n",
       "10002  From: kwilson@casbah.acns.nwu.edu (Kirtley Wil...    6.0   \n",
       "10003  Subject: Re: Don't more innocents die without ...    6.0   \n",
       "10004  From: livesey@solntze.wpd.sgi.com (Jon Livesey...   11.0   \n",
       "10005  From: dls@aeg.dsto.gov.au (David Silver)\\nSubj...   11.0   \n",
       "10006  Subject: Re: Mike Francesa's 1993 Predictions\\...    9.0   \n",
       "10007  From: jet@netcom.Netcom.COM (J. Eric Townsend)...    6.0   \n",
       "10008  From: gld@cunixb.cc.columbia.edu (Gary L Dare)...   18.0   \n",
       "10009  From: sehari@iastate.edu (Babak Sehari)\\nSubje...   11.0   \n",
       "1001   From: danmg@grok85.ColumbiaSC.NCR.COM (Daniel ...   11.0   \n",
       "10010  From: henry@zoo.toronto.edu (Henry Spencer)\\nS...    6.0   \n",
       "10011  From: tzs@stein2.u.washington.edu (Tim Smith)\\...    6.0   \n",
       "10012  From: U56149@uicvm.uic.edu\\nSubject: LCIII & M...   11.0   \n",
       "10013  From: nsmca@aurora.alaska.edu\\nSubject: Lunar ...    6.0   \n",
       "10014  From: acooper@mac.cc.macalstr.edu\\nSubject: Re...   11.0   \n",
       "10015  From: billc@col.hp.com (Bill Claussen)\\nSubjec...    6.0   \n",
       "10016  From: vestman@cs.umu.se (Peter Vestman)\\nSubje...   11.0   \n",
       "10017  From: nstramer@supergas.dazixco.ingr.com (Naft...    6.0   \n",
       "10018  From: kohli@ecs.umass.edu\\nSubject: Mazda GLC ...   11.0   \n",
       "10019  From: mussack@austin.ibm.com (Christopher Muss...    6.0   \n",
       "1002   From: PA146008@utkvm1.utk.edu (David Veal)\\nSu...   11.0   \n",
       "10020  From: noye@midway.uchicago.edu (vera shanti no...    6.0   \n",
       "10021  From:  (Sean Garrison)\\nSubject: Re: WFAN\\nNnt...   11.0   \n",
       "10022  From: christy@cs.concordia.ca (Christy)\\nSubje...    6.0   \n",
       "...                                                  ...    ...   \n",
       "9972   From: rash@access.digex.com (Wayne Rash)\\nSubj...   11.0   \n",
       "9973   From: gt0523e@prism.gatech.EDU (Michael Andre ...   11.0   \n",
       "9974   From: dbernard@clesun.Central.Sun.COM (Dave Be...    6.0   \n",
       "9975   From: smk5@quads.uchicago.edu (Steve Kramarsky...    6.0   \n",
       "9976   From: ob00@ns1.cc.lehigh.edu (OLCAY BOZ)\\nSubj...    6.0   \n",
       "9977   From: sera@zuma.UUCP (Serdar Argic)\\nSubject: ...    6.0   \n",
       "9978   From: moskowit@panix.com (Len Moskowitz)\\nSubj...    6.0   \n",
       "9979   From: rws2v@uvacs.cs.Virginia.EDU (Richard Sto...    6.0   \n",
       "998    From: geb@cs.pitt.edu (Gordon Banks)\\nSubject:...    6.0   \n",
       "9980   From: keith@cco.caltech.edu (Keith Allan Schne...    6.0   \n",
       "9981   From: bredell@tdb.uu.se (Mats Bredell)\\nSubjec...    6.0   \n",
       "9982   From: cantrell@sauron.msfc.nasa.gov (Eric Cant...   11.0   \n",
       "9983   From: cosmo@pro-angmar.alfalfa.com (Frank Bens...   13.0   \n",
       "9984   From: as010b@uhura.cc.rochester.edu (Tree of S...   11.0   \n",
       "9985   From: Thyagi@cup.portal.com (Thyagi Morgoth Na...    6.0   \n",
       "9986   From: leech@cs.unc.edu (Jon Leech)\\nSubject: S...   11.0   \n",
       "9987   From: dmp1@ukc.ac.uk (D.M.Procida)\\nSubject: R...    6.0   \n",
       "9988   From: JBE5 <JBE5@MUSICB.MCGILL.CA>\\nSubject: W...    6.0   \n",
       "9989   From: kurt@oddjob.uchicago.edu (Kurt Henriksen...   11.0   \n",
       "999    From: dxf12@po.cwru.edu (Douglas Fowler)\\nSubj...    6.0   \n",
       "9990   From: dkfox@uxa.cso.uiuc.edu (fox darin k)\\nSu...    6.0   \n",
       "9991   From: ez005997@othello.ucdavis.edu (Oppy)\\nSub...    6.0   \n",
       "9992   From: mont@netcom.com (Mont Pierce)\\nSubject: ...    6.0   \n",
       "9993   From: bskendig@netcom.com (Brian Kendig)\\nSubj...    6.0   \n",
       "9994   From: carl@lvsun.com (Carl Shapiro)\\nSubject: ...    6.0   \n",
       "9995   From: sanjay@kin.lap.upenn.edu (Sanjay Sinha)\\...    6.0   \n",
       "9996   Organization: Central Michigan University\\nFro...    6.0   \n",
       "9997   From: cr292@cleveland.Freenet.Edu (Jim Schenk)...   11.0   \n",
       "9998   From: wbdst+@pitt.edu (William B Dwinnell)\\nSu...   11.0   \n",
       "9999   From: ehung@ampex.com (Eric Hung)\\nSubject: Re...   11.0   \n",
       "\n",
       "                                                  lemmas  \\\n",
       "0      [where, s, thing, car, nntp_posting, host, lin...   \n",
       "1      [guy_kuo, summary, final, call, report, accele...   \n",
       "10     [defence, technology, organisation, line, writ...   \n",
       "100    [earl_wallace, friend, platform, software, lin...   \n",
       "1000   [organization, line, host, write, show, exampl...   \n",
       "10000  [write, help, wife, inform, wants_convertible,...   \n",
       "10001  [steven_walz, many, homosexual, organization, ...   \n",
       "10002  [survivor, reply, distribution, bench, line, w...   \n",
       "10003  [loral, line, write, hate, bike, start, week, ...   \n",
       "10004  [game, game, game, line, electronic, art, disk...   \n",
       "10005  [line, reply, external, wish, startup, disk, p...   \n",
       "10006  [flame, summary, be, outta, begin, week, sabba...   \n",
       "10007  [line, article, write, article, write, questio...   \n",
       "10008  [line, turn, drive, sea, sick, tired, driving,...   \n",
       "10009  [machine, trust, host, com, machine, trial, na...   \n",
       "1001   [lake_steven, wa, line, think, result, last, w...   \n",
       "10010  [people, marry, line, article, write, think, s...   \n",
       "10011  [work, great, article, write, weight_rebound, ...   \n",
       "10012  [host, sydney_australia, line, naive, question...   \n",
       "10013  [interesting, case, school, engineering, line,...   \n",
       "10014  [standing, organization, ericsson_telecom, lin...   \n",
       "10015  [organization, line, reference, line, trim, wr...   \n",
       "10016  [reader, info, particle_physic, line, help, ca...   \n",
       "10017  [year, big, worst_opinion, keyword, article, a...   \n",
       "10018  [periodically, hang, application, openwindow, ...   \n",
       "10019  [insurance, lotsa_point, nntp_posting, host, a...   \n",
       "1002   [region, warrior, ticket, line, distribution, ...   \n",
       "10020  [ear, world, public_access, line, victor_lakin...   \n",
       "10021  [protece, tromsoe, line, be, look, information...   \n",
       "10022  [line, article, write, article, gregg_jaeger, ...   \n",
       "...                                                  ...   \n",
       "9972   [model, organization, line, do, model, pin, ve...   \n",
       "9973   [dept, electrical_engineere, line, add, postag...   \n",
       "9974   [gun, advertisement, organization, laboratory,...   \n",
       "9975   [documentation, article, line, find, believe, ...   \n",
       "9976   [people, become, atheist, reply, write, articl...   \n",
       "9977   [douglas_rand, draw, line, inverse, message, o...   \n",
       "9978   [organization, apply, line, follow, comment, a...   \n",
       "9979   [article, write, write, tiny, area, land, high...   \n",
       "998    [write, article, write, friend, connect, use, ...   \n",
       "9980   [muslim, cruelly, bayonet, death, reply, serda...   \n",
       "9981   [update, organization, write, result, day, vot...   \n",
       "9982   [s, wrong, host, have, also, find, electronic,...   \n",
       "9983   [standing, organization, hewlett_packard, line...   \n",
       "9984   [lesc, line, host, antare, ncd, exceed, softwa...   \n",
       "9985   [need, cheap, drug, line, write, answer, seem,...   \n",
       "9986   [want, super, projector, sound, projector, nnt...   \n",
       "9987   [line, article, write, write, nuclear, fission...   \n",
       "9988   [armenian, problem, write, tell, follow, story...   \n",
       "9989   [problem, organization, western_geophysical, e...   \n",
       "999    [clipper_chip, crypto, organization, line, hos...   \n",
       "9990   [love, royal_road, military, line, article, wr...   \n",
       "9991   [article, write, way, want, however, follow, p...   \n",
       "9992   [signal_surge, home, nntp_posting, host, write...   \n",
       "9993   [school, engineering, line, host, useragent_nu...   \n",
       "9994   [organization, mantis_consultant, line, write,...   \n",
       "9995   [gordon_bank, gordon, objection, article, writ...   \n",
       "9996   [article, line, need, shelf, method, transmit,...   \n",
       "9997   [line, think, get, consumer_report, rank, high...   \n",
       "9998   [line, sale, fit, car, cover, specifically, to...   \n",
       "9999   [line, allegro, sophisticated, design, straigh...   \n",
       "\n",
       "                                            tf_idf_words  \n",
       "0               [car, door, lerxst, funky, front_bumper]  \n",
       "1             [upgrade, clock, guy_kuo, add, experience]  \n",
       "10     [fractally, recognisable, tv_station, organisa...  \n",
       "100          [gun, people, bulky, allowed, earl_wallace]  \n",
       "1000   [domestication, behavior, wild, domesticate, a...  \n",
       "10000                [convertible, car, wife, range, ds]  \n",
       "10001               [law, charge, beat, deter, straight]  \n",
       "10002              [blast, fatigue, people, knock, hell]  \n",
       "10003          [plastic, compound, rubbing, work, loral]  \n",
       "10004      [game, best_offer, art, electronic, original]  \n",
       "10005             [boot, external, warm, startup, reply]  \n",
       "10006                    [ride, week, fly, cfc, newsfee]  \n",
       "10007  [redneck, fascinate, ride, spluttering, southe...  \n",
       "10008    [palestinian, islamic, situation, find, choice]  \n",
       "10009      [imagine, machine, nanosecond, mjr, universe]  \n",
       "1001             [mark, cheryl, banner, lake_steven, wa]  \n",
       "10010       [couple, marry, marriage, priest, community]  \n",
       "10011  [weight, term, lose, weight_rebound, constitut...  \n",
       "10012     [computationally, intensive, job, claim, lino]  \n",
       "10013               [mouse, quirk, speed, style, normal]  \n",
       "10014  [relegation, quarterfinal, standing, swedish_h...  \n",
       "10015            [murder, moral, system, morality, goal]  \n",
       "10016  [reader, connector, locally, particle_physic, ...  \n",
       "10017       [season, team, goal, point, hartford_whaler]  \n",
       "10018  [application, openwindow, click, pulldown, ser...  \n",
       "10019         [drunk, insure, drive, insurance, vehicle]  \n",
       "1002            [ticket, warrior, opponent, row, region]  \n",
       "10020         [sighting, planet, einstein, ear, explain]  \n",
       "10021        [proteced, protece, tromsoe, mode, support]  \n",
       "10022  [islamic, statement, country, rushdie, publica...  \n",
       "...                                                  ...  \n",
       "9972                      [model, do, pin, job, already]  \n",
       "9973   [eastw, electrical_engineere, shrink, disc, po...  \n",
       "9974          [ad, sportshop, crpa, sportsmen, examiner]  \n",
       "9975   [chak, undocumented, legend, diagnostic, mapping]  \n",
       "9976     [atheist, religion, belief, hostility, believe]  \n",
       "9977             [gxxor, inverse, draw, terminal, white]  \n",
       "9978                  [cdt, koresh, claim, clh, prophet]  \n",
       "9979   [population_density, statement, gaza_strip, po...  \n",
       "998    [procomm_plus, baud_modem, trusty, menzel, mac...  \n",
       "9980    [armenian, genocide, massacre, russian, turkish]  \n",
       "9981      [vote, mould, valerie_hammerl, voting, philly]  \n",
       "9982             [fixture, nuke, instant, lamp, starter]  \n",
       "9983                [blue, dirty, team, standing, white]  \n",
       "9984      [antare, lesc, exceed, ncd, opinions_expresse]  \n",
       "9985   [cheap, crypto, birthright, clipper_chip, stro...  \n",
       "9986              [projector, sound, super, sale, danny]  \n",
       "9987           [uranium, cooling_tower, air, mist, melt]  \n",
       "9988   [persian, persian_troop, persian_army, fourth,...  \n",
       "9989   [doug, libtermcap_termcap, exploration_product...  \n",
       "999     [encryption, scrambled, technology, wedge, lull]  \n",
       "9990                        [seal, btw, faith, dk, took]  \n",
       "9991                   [helmet, large, hang, chin, size]  \n",
       "9992   [unlikely, possible, signal_surge, alternator,...  \n",
       "9993         [defect, manufacture, monitor, new, school]  \n",
       "9994   [recursive, recursiveness, iterative, optimizi...  \n",
       "9995             [gordon_bank, sin, blood, mormon, save]  \n",
       "9996   [transmitting, product, prefered, appreciate_h...  \n",
       "9997   [car, deal, fully_loade, catagorie, honda_accord]  \n",
       "9998            [cover, fabric, grommet, blemish, stain]  \n",
       "9999      [sheet, sophisticated, cod, preference, never]  \n",
       "\n",
       "[11314 rows x 4 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coherence score** измеряет оценку одного топика с помощью измерения степени семантического сходства между словами, которые были оценены высоко в этом топике (то есть считаются \"релевантными\"). \n",
    "**Coherence score типа c_v** работает на основе метода скользящего окна (\"алгоритм трансформации, позволяющий сформировать из членов временного ряда набор данных, который может служить обучающим множеством для построения модели прогнозирования\" (https://wiki.loginom.ru/articles/windowing-method.html)), сегментации наиболее релевантных слов и мере, которая использует нормализованную точечную взаимную информацию (NPMI)(https://en.wikipedia.org/wiki/Pointwise_mutual_information) и косинусное сходство.\n",
    "Источник: https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
